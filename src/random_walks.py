import networkx as nx
import numpy as np
from numpy.random import default_rng
#import torch as th
from torch.utils.data import IterableDataset#, get_worker_info
from typing import Iterator



def one_hot_encoder(label: int, length: int) -> np.ndarray:
    '''returns one-hot vector according to given label integer and length'''
    zero_vector = np.zeros(length)
    zero_vector[label] = 1
    return zero_vector



class RW_Iterable(IterableDataset):
    '''implements custom iterable dataset for random [pq]-walks of length l over given [graph], together w/ [l_ns] negative samples'''
    def __init__(self, graph: nx.Graph, p: float, q: float, l: int, l_ns: int, batch_size: int) -> None:
        super().__init__()

        # main graph attributes
        self.graph = graph  # full nx.Graph
        self.adj_mat = nx.adjacency_matrix(graph).toarray()  # int32-np.ndarray
        #self.adj_mat = th.tensor(nx.adjacency_matrix(graph).toarray())  # int32-th.tensor
        self.n_nodes = nx.number_of_nodes(graph)
        self.n_edges = nx.number_of_edges(graph)  # optional?

        self.node_labels = [node[1]['node_label'] for node in graph.nodes(data=True)]  # int-list, fails for Facebook, idk why, not needed tho
        # one-hot-encoded [node_labels], n_nodes x label_range
        self.min_label = min(self.node_labels)
        self.label_range = max(self.node_labels) - self.min_label + 1
        self.node_labels_one_hot = np.array([one_hot_encoder(label - self.min_label, self.label_range) for label in self.node_labels])
        #self.node_labels_one_hot = th.tensor(self.node_labels_one_hot)  #th.tensor

        self.nodes_start = [edge[0] - 1 for edge in graph.edges(data=True)]  # subtract 1 to account for node count starting at zero
        self.nodes_end = [edge[1] - 1 for edge in graph.edges(data=True)]
        self.edges = [self.nodes_start, self.nodes_end]  # int-list, 2 x n_edges
        #self.edges = th.tensor(self.edges)  # th.tensor

        # attributes for random walk generation
        self.rng = default_rng(seed=None)  # default RNG object for general random sampling
        self.p = p
        self.q = q
        self.l = l  # random walk length (excl. start node)
        self.l_ns = l_ns  # number of negative samples

        # attributes for batching & worker split
        self.batch_size = batch_size  # size of random walk batch generated by rw_batch()
        #self.start = 0
        #self.end = batch_size


    def rw_batch(self) -> list[np.ndarray]:
        '''returns batch (list) of pq-walk data, each including: random start node, followed by l nodes of random pq-walk, followed by l_ns negative samples, concatenated into 1D-np.ndarray'''
        batch = []  # collect pq-walk data arrays in list
        for walk in range(self.batch_size):
            # last node, initially the uniformly sampled start node
            #last = self.rng.integers(0, high=self.n_nodes, size=None, dtype=np.int64, endpoint=False)
            last = self.rng.choice(self.n_nodes, size=None, replace=True, p=None, axis=0, shuffle=True)  # np.int32
            start_nbh = self.adj_mat[last]  # neighborhood of start node repres. as resp. row of adjacency matrix
            # current node, initially the 2nd node of pq-walk, uniformly sampled from neighborhood of start node
            current = self.rng.choice(self.n_nodes, size=None, replace=True, p = start_nbh / np.sum(start_nbh), axis=0, shuffle=True)
            pq_walk = [last, current]  # collect sampled nodes of pq-walk in list

            for step in range(self.l - 1):  # sample the l-1 next nodes in pq-walk using algebraic construction of alpha (see def. in script/sheet)
                current_nbh = self.adj_mat[current]  # neighborhood of current node repres. as its adj.mat.row
                # common neighborhood of last & current node, repres. as elem-wise product of resp. adj.mat.rows, accounts for 2nd row in def. of alpha
                common_nbh = np.multiply(self.adj_mat[last], current_nbh)
                # alpha repres. as array of disc. probab.s over all nodes (not norm.ed), hence the use of adj.mat.rows to represent neighborhoods
                alpha = common_nbh + (current_nbh - common_nbh) / self.q  # accounts for 1st & 2nd row in def. of alpha
                alpha[last] = 1 / self.p  # accounts for 3rd row in def. of alpha (step back to last node)

                # sample next node in pq-walk according to norm.ed alpha (discrete probab. over all nodes)
                next = self.rng.choice(self.n_nodes, size=None, replace=True, p = alpha / np.sum(alpha), axis=0, shuffle=True)
                pq_walk.append(next)

                # update last & current node
                last = current
                current = next

            # negative samples (np.ndarray) uniformly drawn from remaining nodes w/o repetition
            neg_samples = self.rng.choice(list(set(range(self.n_nodes)).difference(set(pq_walk))), size=self.l_ns, replace=False, p=None, axis=0, shuffle=False)

            #batch.append(th.tensor(np.concatenate([np.array(pq_walk), neg_samples], axis=-1)))
            batch.append(np.concatenate([np.array(pq_walk), neg_samples], axis=-1))  # gets cast to th.tensor by dataloader

        return batch  # dataset supposed to be same size as batch here


    def __iter__(self) -> Iterator[np.ndarray]:
        '''returns iterator of pq-walk data, single-process: runs slow, unreliable or unintended otherwise...'''
        return iter(self.rw_batch())



if __name__ == "__main__":
    # for testing streaming, batching, multiprocessing, etc.
    import pickle
    from psutil import cpu_count
    from torch.utils.data import DataLoader, get_worker_info
    """
    def worker_split(worker_id) -> None:
        worker_info = get_worker_info()
        ds = worker_info.dataset  # the dataset copy in this worker process
        overall_start = ds.start
        overall_end = ds.end
        # configure the dataset to only process the split workload
        per_worker = int(np.ceil((overall_end - overall_start) / worker_info.num_workers))
        #worker_id = worker_info.id
        ds.start = overall_start + worker_info.id * per_worker
        #ds.start = overall_start + worker_id * per_worker
        ds.end = min(ds.start + per_worker, overall_end)
    """
    with open('datasets/Citeseer/data.pkl', 'rb') as data:
    #with open('datasets/Cora/data.pkl', 'rb') as data:
    #with open('datasets/Facebook/data.pkl', 'rb') as data:  # cannot construct self.node_labels for Facebook, idk why, not needed tho
    #with open('datasets/PPI/data.pkl', 'rb') as data:
        graphs = pickle.load(data)
    graph = graphs[0]

    n_workers = 2 #cpu_count(logical=True)
    batch_size =  5 #3 * n_workers
    dataset = RW_Iterable(graph, p=0.5, q=0.5, l=5, l_ns=5, batch_size=batch_size)  # custom iterable dataset instance
    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0)  # turns random walk batches into th.tensors, single-process w/ renewing randomness

    # multi-process part removed
    #dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers, worker_init_fn=worker_split)  # issue w/ worker_split(), produces failed subprocesses
    #dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers)  # works but returns one batch for each subprocess? also duplicates depending on implementation (expected but not efficiently fixed yet)

    for run in range(3):
        for batch in dataloader:
            print(batch)
        print("\n")
